@inproceedings{Batra2008,
abstract = {Spectral clustering and eigenvector-based methods have become increasingly popular in segmentation and recognition. Although the choice of the pairwise similarity metric (or affinities) greatly influences the quality of the results, this choice is typically specified outside the learning framework. In this paper, we present an algorithm to learn class-specific similarity functions. Mapping our problem in a conditional random fields (CRF) framework enables us to pose the task of learning affinities as parameter learning in undirected graphical models. There are two significant advances over previous work. First, we learn the affinity between a pair of data-points as a function of a pairwise feature and (in contrast with previous approaches) the classes to which these two data-points were mapped, allowing us to work with a richer class of affinities. Second, our formulation provides a principled probabilistic interpretation for learning all of the parameters that define these affinities. Using ground truth segmentations and labellings for training, we learn the parameters with the greatest discriminative power (in an MLE sense) on the training data. We demonstrate the power of this learning algorithm in the setting of joint segmentation and recognition of object classes. Specifically, even with very simple appearance features, the proposed method achieves state-of-the-art performance on standard datasets.},
author = {Batra, Dhruv and Sukthankar, Rahul and Chen, Tsuhan},
booktitle = {Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2008.4587432},
file = {:home/ethiy/Documents/Mendeley Desktop/2008/Batra, Sukthankar, Chen/Batra, Sukthankar, Chen{\_}2008{\_}Learning class-specific affinities for image labelling.pdf:pdf},
isbn = {9781424422432},
issn = {1063-6919},
mendeley-groups = {Labelization},
title = {{Learning class-specific affinities for image labelling}},
year = {2008}
}
@inproceedings{Biswas2012,
abstract = {We propose a method of clustering images that combines algorithmic and human input. An algorithm provides us with pairwise image similarities. We then actively obtain selected, more accurate pairwise similarities from humans. A novel method is developed to choose the most useful pairs to show a person, obtaining constraints that improve clus- tering. In a clustering assignment elements in each data pair are either in the same cluster or in different clusters. We simulate inverting these pairwise relations and see how that affects the overall clustering. We choose a pair that maximizes the expected change in the clustering. The proposed algorithm has high time complexity, so we also propose a version of this algorithm that is much faster and exactly replicates our original algorithm. We further im- prove run-time by adding heuristics, and show that these do not significantly impact the effectiveness of our method. We have run experiments in two different domains, namely leaf images and face images, and show that clustering perfor- mance can be improved significantly.},
author = {Biswas, Arijit and Jacobs, David},
booktitle = {Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247922},
file = {:home/ethiy/Documents/Mendeley Desktop/2012/Biswas, Jacobs/Biswas, Jacobs{\_}2012{\_}Active image clustering Seeking constraints from humans to complement algorithms.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
mendeley-groups = {Labelization},
pages = {2152--2159},
title = {{Active image clustering: Seeking constraints from humans to complement algorithms}},
year = {2012}
}
@article{Galleguillos2014,
abstract = {The goal of an object category discovery system is to annotate a pool of unlabeled image data, where the set of labels is initially unknown to the system, and must therefore be discovered over time by querying a human annotator. The annotated data is then used to train object detectors in a standard supervised learning setting, possibly in conjunction with category discovery itself. Category discovery systems can be evaluated in terms of both accuracy of the resulting object detectors, and the efficiency with which they discover categories and annotate the training data. To improve the accuracy and efficiency of category discovery, we propose an iterative framework which alternates between optimizing nearest neighbor classification for known categories with multiple kernel metric learning, and detecting clusters of unlabeled image regions likely to belong to a novel, unknown categories. Experimental results on the MSRC and PASCAL VOC2007 data sets show that the proposed method improves clustering for category discovery, and efficiently annotates image regions belonging to the discovered classes. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Galleguillos, Carolina and McFee, Brian and Lanckriet, Gert R.G.},
doi = {10.1007/s11263-013-0679-z},
file = {:home/ethiy/Documents/Mendeley Desktop/2014/Galleguillos, McFee, Lanckriet/Galleguillos, McFee, Lanckriet{\_}2014{\_}Iterative category discovery via multiple kernel metric learning.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Category discovery,Iterative discovery,Metric learning,Multiple kernel learning},
mendeley-groups = {Labelization},
number = {1-2},
pages = {115--132},
title = {{Iterative category discovery via multiple kernel metric learning}},
volume = {108},
year = {2014}
}
@inproceedings{Gilbert2011,
abstract = {ly developed for text analysis; min-Hash and APriori are used and extended to achieve both speed and scala- bility on large image and video datasets. Inspired by the Bag-of-Words (BoW) architecture, the idea of an image sig- nature is introduced as a simple descriptor on which near- est neighbour classification can be performed. The image signature is then dynamically expanded to identify common features amongst samples of the same class. The iterative approach uses APriori to identify common and distinctive elements of a small set of labelled true and false positive signatures. These elements are then accentuated in the sig- nature to increase similarity between examples and “pull” positive classes together. By repeating this process, the ac- curacy of similarity increases dramatically despite only a few training examples, only 10{\%} of the labelled groundtruth is needed, compared to other approaches. It is tested on two image datasets including the caltech101 dataset and on three state-of-the-art action recognition datasets. On the YouTube video dataset the accuracy increases from 72{\%} to 97{\%} using only 44 labelled examples from a dataset of over 1200 videos. The approach is both scalable and ef- ficient, with an iteration on the full YouTube dataset taking around 1 minute on a standard desktop machine.},
author = {Gilbert, Andrew and Bowden, Richard},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126493},
file = {:home/ethiy/Documents/Mendeley Desktop/2011/Gilbert, Bowden/Gilbert, Bowden{\_}2011{\_}iGroup Weakly supervised image and video grouping.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
mendeley-groups = {Labelization},
number = {June 2014},
pages = {2166--2173},
title = {{iGroup: Weakly supervised image and video grouping}},
year = {2011}
}
@inproceedings{Lee2011,
abstract = {Objects vary in their visual complexity, yet existing dis-covery methods perform " batch " clustering, paying equal attention to all instances simultaneously—regardless of the strength of their appearance or context cues. We propose a self-paced approach that instead focuses on the easiest instances first, and progressively expands its repertoire to include more complex objects. Easier regions are defined as those with both high likelihood of generic objectness and high familiarity of surrounding objects. At each cycle of the discovery process, we re-estimate the easiness of each subwindow in the pool of unlabeled images, and then re-trieve a single prominent cluster from among the easiest instances. Critically, as the system gradually accumulates models, each new (more difficult) discovery benefits from the context provided by earlier discoveries. Our experi-ments demonstrate the clear advantages of self-paced dis-covery relative to conventional batch approaches, including both more accurate summarization as well as stronger pre-dictive models for novel data.},
author = {Lee, Yong Jae and Grauman, Kristen},
booktitle = {Conference on Computer Vision and Pattern Recognition},
file = {:home/ethiy/Documents/Mendeley Desktop/2011/Lee, Grauman/Lee, Grauman{\_}2011{\_}Learning the Easy Things First Self-Paced Visual Category Discovery.pdf:pdf},
mendeley-groups = {Labelization},
title = {{Learning the Easy Things First: Self-Paced Visual Category Discovery}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.362.7280{\&}rep=rep1{\&}type=pdf},
year = {2011}
}
@article{Pordel2015,
abstract = {Image labeling tools help to extract objects within images to be used as ground truth for learning and testing in object detection processes. The inputs for such tools are usually RGB images. However with new widely available low-cost sensors like Microsoft Kinect it is possible to use depth images in addition to RGB images. Despite many existing powerful tools for image labeling, there is a need for RGB-depth adapted tools. We present a new interactive labeling tool that partially automates image labeling, with two major contributions. First, the method extends the concept of image segmentation from RGB to RGB-depth using Fuzzy C-Means clustering, connected component labeling and superpixels, and generates bounding pixels to extract the desired objects. Second, it minimizes the interaction time needed for object extraction by doing an efficient segmentation in RGB-depth space. Very few clicks are needed for the entire procedure compared to existing, tools. When the desired object is the closest object to the camera, which is often the case in robotics applications, no clicks at all are required to accurately extract the object.},
author = {Pordel, Mostafa and Hellstr{\"{o}}m, Thomas},
doi = {10.3390/computers4020142},
file = {:home/ethiy/Documents/Mendeley Desktop/2015/Pordel, Hellstr{\"{o}}m/Pordel, Hellstr{\"{o}}m{\_}2015{\_}Semi-Automatic Image Labelling Using Depth Information.pdf:pdf},
issn = {2073-431X},
journal = {Computers},
keywords = {Microsoft Kinect,RGBD data,depth information,image labelling,image segmentation,labelling tools,object detection,robot vision},
mendeley-groups = {Labelization},
number = {2},
pages = {142--154},
title = {{Semi-Automatic Image Labelling Using Depth Information}},
url = {http://www.mdpi.com/2073-431X/4/2/142/{\%}5Cnhttp://www.mdpi.com/2073-431X/4/2/142/htm},
volume = {4},
year = {2015}
}
@inproceedings{Vijayanarasimhan2010,
abstract = {Active learning methods aim to select the most informative unlabeled instances to label first, and can help to focus image or video annotations on the examples that will most improve a recognition system. However, most existing methods only make myopic queries for a single label at a time, retraining at each iteration. We consider the problem where at each iteration the active learner must select a set of examples meeting a given budget of supervision, where the budget is determined by the funds (or time) available to spend on annotation. We formulate the budgeted selection task as a continuous optimization problem where we determine which subset of possible queries should maximize the improvement to the classifier's objective, without overspending the budget. To ensure far-sighted batch requests, we show how to incorporate the predicted change in the model that the candidate examples will induce. We demonstrate the proposed algorithm on three datasets for object recognition, activity recognition, and content-based retrieval, and we show its clear practical advantages over random, myopic, and batch selection baselines.},
author = {Vijayanarasimhan, Sudheendra and Jain, Prateek and Grauman, Kristen},
booktitle = {Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540055},
file = {:home/ethiy/Documents/Mendeley Desktop/2010/Vijayanarasimhan, Jain, Grauman/Vijayanarasimhan, Jain, Grauman{\_}2010{\_}Far-sighted active learning on a budget for image and video recognition.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
mendeley-groups = {Labelization},
number = {June},
pages = {3035--3042},
title = {{Far-sighted active learning on a budget for image and video recognition}},
year = {2010}
}
@article{Wigness2017,
abstract = {Raw visual data used to train classifiers is abundant and easy to gather, but lacks semantic labels that describe visual concepts of interest. These labels are necessary for supervised learning and can require significant human effort to collect. We discuss four labeling objectives that play an important role in the design of frameworks aimed at collecting label information for large training sets while maintaining low human effort: discovery, efficiency, exploitation and accuracy. We introduce a framework that explicitly models and balances these four labeling objectives with the use of (1) hierarchical clustering, (2) a novel interestingness measure that defines structural change within the hierarchy, and (3) an iterative group-based labeling process that exploits relationships between labeled and unlabeled data. Results on benchmark data show that our framework collects labeled training data more efficiently than existing labeling techniques and trains higher performing visual classifiers. Further, we show that our resulting framework is fast and significantly reduces human interaction time when labeling real-world multi-concept imagery depicting outdoor environments.},
author = {Wigness, Maggie and Draper, Bruce A. and Beveridge, J. Ross},
doi = {10.1007/s11263-017-1039-1},
file = {:home/ethiy/Documents/Mendeley Desktop/2017/Wigness, Draper, Beveridge/Wigness, Draper, Beveridge{\_}2017{\_}Efficient Label Collection for Image Datasets via Hierarchical Clustering.pdf:pdf},
isbn = {9781467369640},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Efficient label collection,Hierarchical clustering,Image classification,Visual concept discovery},
mendeley-groups = {Labelization},
pages = {1--27},
publisher = {Springer US},
title = {{Efficient Label Collection for Image Datasets via Hierarchical Clustering}},
url = { https://link.springer.com/content/pdf/10.1007{\%}2Fs11263-017-1039-1.pdf},
year = {2017}
}
@inproceedings{Wigness2014,
abstract = {Labeling data to train visual concept classifiers requires significant human effort. Active learning addresses labeling overhead by selecting a meaningful subset of data, but often these approaches assume that the set of visual concepts is known in advance. Clustering approaches perform bottom-up discovery of concepts, and reduce labeling effort by moving from instance-based to group-based labeling. Unfortunately, clustering techniques assume a one-to-one mapping between clusters and visual concepts even though learned groups are often not coherent and fail to represent all concepts. We introduce Selective Guidance, a technique that hierarchically clusters data and selectively queries labels of coherent clusters representing different visual concepts. Unlike most active learning and clustering techniques, Selective Guidance does not require any a priori knowledge. Using benchmark data sets we show that Selective Guidance achieves classification accuracy better than active learning and clustering approaches with fewer labeling. queries. {\textcopyright} 2014 IEEE.},
author = {Wigness, Maggie and Draper, Bruce A. and Beveridge, J. Ross},
booktitle = {Winter Conference on Applications of Computer Vision},
doi = {10.1109/WACV.2014.6836093},
file = {:home/ethiy/Documents/Mendeley Desktop/2014/Wigness, Draper, Beveridge/Wigness, Draper, Beveridge{\_}2014{\_}Selectively guiding visual concept discovery.pdf:pdf},
isbn = {9781479949854},
mendeley-groups = {Labelization},
number = {March},
pages = {247--254},
title = {{Selectively guiding visual concept discovery}},
year = {2014}
}
@inproceedings{Wigness2015,
abstract = {Visual classifiers are part of many applications includ- ing surveillance, autonomous navigation and scene under- standing. The raw data used to train these classifiers is abundant and easy to collect but lacks labels. Labels are necessary for training supervised classifiers, but the label- ing process requires significant human effort. Techniques like active learning and group-based labeling have emerged to help reduce the labeling workload. However, the possi- bility of collecting label noise affects either the efficiency of these systems or the performance of the trained clas- sifiers. Further, many introduce latency by iteratively retraining classifiers or re-clustering data. We introduce a technique that searches for structural change in hierarchically clustered data to identify a set of clusters that span a spectrum of visual concept granularities. This allows us to efficiently label clusters with less label noise and produce high performing classifiers. The data is hierarchically clus- tered only once, eliminating latency during the labeling process. Using benchmark data we show that collecting labels with our approach is more efficient than existing labeling techniques, and achieves higher classification accuracy. Finally, we demonstrate the speed and efficiency of our system using real-world data collected for an autonomous naviga- tion task.},
author = {Wigness, Maggie and Draper, Bruce A. and Beveridge, J. Ross},
booktitle = {Conference on Computer Vision and Pattern Recognition},
doi = {10.1007/s11263-017-1039-1},
file = {:home/ethiy/Documents/Mendeley Desktop/2015/Wigness, Draper, Beveridge/Wigness, Draper, Beveridge{\_}2015{\_}Efficient Label Collection for Unlabeled Image Datasets.pdf:pdf},
isbn = {9781467369640},
issn = {15731405},
keywords = {Efficient label collection,Hierarchical clustering,Image classification,Visual concept discovery},
mendeley-groups = {Labelization},
pages = {1--27},
title = {{Efficient Label Collection for Unlabeled Image Datasets}},
year = {2015}
}
@inproceedings{Wigness2016,
abstract = {Multi-concept visual classification is emerging as a common environment perception technique, with applications in autonomous mobile robot navigation. Supervised visual classifiers are typically trained with large sets of images, hand annotated by humans with region boundary outlines followed by label assignment. This annotation is time consuming, and unfortunately, a change in environment requires new or additional labeling to adapt visual perception. The time is takes for a human to label new data is what we call adaptation latency. High adaptation latency is not simply undesirable but may be infeasible for scenarios with limited labeling time and resources. In this paper, we introduce a labeling framework to the environment perception domain that significantly reduces adaptation latency using unsupervised learning in exchange for a small amount of label noise. Using two real-world datasets we demonstrate the speed of our labeling framework, and its ability to collect environment labels that train high performing multi-concept classifiers. Finally, we demonstrate the relevance of this label collection process for visual perception as it applies to navigation in outdoor environments.},
author = {Wigness, Maggie and Rogers, John G. and Navarro-Serment, Luis Ernesto and Suppe, Arne and Draper, Bruce A.},
booktitle = {International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2016.7759432},
file = {:home/ethiy/Documents/Mendeley Desktop/2016/Wigness et al/Wigness et al.{\_}2016{\_}Reducing adaptation latency for multi-concept visual perception in outdoor environments.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
mendeley-groups = {Labelization},
pages = {2784--2791},
title = {{Reducing adaptation latency for multi-concept visual perception in outdoor environments}},
volume = {2016-Novem},
year = {2016}
}

